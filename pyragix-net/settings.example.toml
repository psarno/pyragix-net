# PyRagix Configuration
# Local-first RAG system with hybrid search, semantic chunking, and query expansion
# All values are local - no external transmission. Stable TOML format.

# ============================================================================
# CPU & THREADING
# ============================================================================
# Tuned for 16GB RAM / 6GB VRAM laptop. Adjust based on your hardware.

[threading]
TORCH_NUM_THREADS = 6          # PyTorch threading (CPU)
OPENBLAS_NUM_THREADS = 6       # OpenBLAS threading (linear algebra)
MKL_NUM_THREADS = 6            # MKL threading (Intel Math Kernel Library)
OMP_NUM_THREADS = 6            # OpenMP threading (parallelization)
NUMEXPR_MAX_THREADS = 6        # NumExpr threading (fast expression evaluation)

# ============================================================================
# GPU / CUDA SETTINGS
# ============================================================================
# Requires CUDA-capable GPU. PyTorch auto-detects CUDA availability.

[gpu]
CUDA_VISIBLE_DEVICES = "0"     # GPU device ID (0 = first GPU, "0,1" = multiple)
PYTORCH_CUDA_ALLOC_CONF = "max_split_size_mb:1024,garbage_collection_threshold:0.9"
FAISS_DISABLE_CPU = "1"        # Force FAISS to prefer GPU (when GPU available)
CUDA_LAUNCH_BLOCKING = "0"     # Allow async CUDA operations for performance

# FAISS GPU acceleration (advanced users only - requires faiss-gpu from conda-forge)
GPU_ENABLED = false            # Most users should leave false (CPU is stable)
GPU_DEVICE = 0                 # GPU device ID for FAISS
GPU_MEMORY_FRACTION = 0.8      # Fraction of GPU memory to use (0.0-1.0)

# ============================================================================
# EMBEDDING & BATCH SETTINGS
# ============================================================================
# Sentence-transformers and FAISS batch configuration.

[embeddings]
BATCH_SIZE = 16                # Batch size for embedding generation (higher = faster but more VRAM)
EMBED_MODEL = "all-MiniLM-L6-v2"  # HuggingFace sentence-transformer model

# ============================================================================
# FAISS VECTOR INDEX SETTINGS
# ============================================================================
# FAISS (Facebook AI Similarity Search) for fast semantic similarity.

[faiss]
INDEX_TYPE = "ivf"             # Index type: "flat" (exact), "ivf" (faster, ~99% recall), "ivf_pq" (smaller)
NLIST = 1024                   # Number of clusters (for ivf/ivf_pq, higher = slower but more accurate)
NPROBE = 16                    # Number of clusters to search (higher = slower but more accurate)

# ============================================================================
# PDF PROCESSING SETTINGS
# ============================================================================
# Settings for OCR, rendering, and document extraction.

[pdf]
BASE_DPI = 150                 # DPI for PDF page rendering to images (higher = better quality but slower)
BATCH_SIZE_RETRY_DIVISOR = 4   # Divisor for reducing batch size when memory errors occur
SKIP_FILES = []                # List of filenames to skip during processing (e.g., ["temp.pdf", "ignore.txt"])

# ============================================================================
# FILE PATHS & LOGGING
# ============================================================================
# Output file locations for logs and database.

[files]
INGESTION_LOG_FILE = "ingestion.log"  # Log file for document ingestion process
CRASH_LOG_FILE = "crash_log.txt"      # Log file for crash reports

# ============================================================================
# OLLAMA / LLM SETTINGS
# ============================================================================
# Local LLM via Ollama. Ensure ollama serve is running before querying.
# Download models with: ollama pull qwen2.5:7b

[llm]
OLLAMA_BASE_URL = "http://localhost:11434"  # Ollama API endpoint
OLLAMA_MODEL = "qwen2.5:7b"                 # Model name (must be installed in Ollama)
REQUEST_TIMEOUT = 180                       # Timeout in seconds for LLM requests
TEMPERATURE = 0.1                           # LLM temperature (0.0=deterministic, 2.0=creative)
TOP_P = 0.9                                 # LLM top-p sampling (nucleus sampling)
MAX_TOKENS = 500                            # Maximum tokens to generate in response

# ============================================================================
# RETRIEVAL SETTINGS
# ============================================================================
# Controls how many documents are retrieved and returned.

[retrieval]
DEFAULT_TOP_K = 7              # Number of top documents to retrieve for each query

# ============================================================================
# PHASE 1: QUERY EXPANSION
# ============================================================================
# Multi-query expansion for improved recall. Generates query variants.
# Trades speed for higher recall (finds more relevant documents).

[query_expansion]
ENABLE_QUERY_EXPANSION = true  # Enable multi-query expansion
QUERY_EXPANSION_COUNT = 3      # Number of query variants to generate

# ============================================================================
# PHASE 1: CROSS-ENCODER RERANKING
# ============================================================================
# Cross-encoder for precision ranking. Re-scores top-K results for accuracy.
# Trades speed for higher precision (more relevant results at top).

[reranking]
ENABLE_RERANKING = true                              # Enable cross-encoder reranking
RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"  # Cross-encoder model
RERANK_TOP_K = 20                                    # Retrieve this many before reranking

# ============================================================================
# PHASE 2: HYBRID SEARCH
# ============================================================================
# Combines semantic (FAISS) + keyword (BM25) search for balanced results.
# FAISS finds semantically similar, BM25 finds keyword matches.

[hybrid_search]
ENABLE_HYBRID_SEARCH = true    # Enable hybrid FAISS + BM25 search
HYBRID_ALPHA = 0.7             # Weight for FAISS (0.7 = 70% semantic + 30% keyword)
BM25_INDEX_PATH = "bm25_index.pkl"  # Path to BM25 index file

# ============================================================================
# PHASE 3: SEMANTIC CHUNKING
# ============================================================================
# Sentence-boundary-aware chunking instead of fixed-size chunks.
# Respects sentence boundaries for better semantic coherence.

[semantic_chunking]
ENABLE_SEMANTIC_CHUNKING = true  # Enable sentence-aware chunking
SEMANTIC_CHUNK_MAX_SIZE = 1600   # Maximum characters per chunk
SEMANTIC_CHUNK_OVERLAP = 200     # Character overlap between chunks
